{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d783e1f5",
   "metadata": {},
   "source": [
    "# Hypothesis Testing: The Framework of Statistical Significance\n",
    "\n",
    "**Hypothesis Testing** is a systematic way to test claims about a population based on sample data. It allows us to move beyond \"guessing\" and determine if the patterns we see in data are truly meaningful.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Four Essential Components\n",
    "\n",
    "### A. The Hypotheses\n",
    "* **Null Hypothesis ($H_0$):** The \"Status Quo.\" It assumes there is no effect, no difference, or no relationship. \n",
    "    * *Example: \"The new website design has no effect on sales.\"*\n",
    "* **Alternative Hypothesis ($H_a$ or $H_1$):** What you are trying to prove. It assumes there is a significant effect or difference.\n",
    "    * *Example: \"The new website design increases sales.\"*\n",
    "\n",
    "### B. The P-Value\n",
    "The **P-value** is the probability of seeing a result as extreme as yours (or more extreme) if the Null Hypothesis were true.\n",
    "* **Small P-value ($\\le 0.05$):** The result is unlikely to be luck; we reject $H_0$.\n",
    "* **Large P-value ($> 0.05$):** The result could easily be luck; we fail to reject $H_0$.\n",
    "\n",
    "\n",
    "\n",
    "### C. Significance Level ($\\alpha$)\n",
    "The threshold for \"proof,\" usually set at **0.05**. If your P-value is lower than $\\alpha$, your result is \"Statistically Significant.\"\n",
    "\n",
    "### D. Test Statistic\n",
    "A numerical value calculated from your data (like a **Z-score** or **T-score**) that measures how far your sample diverges from the Null Hypothesis.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Steps to Perform a Hypothesis Test\n",
    "1.  **State the Hypotheses:** Define $H_0$ and $H_a$.\n",
    "2.  **Choose $\\alpha$:** Usually $0.05$ (5% risk of being wrong).\n",
    "3.  **Collect Data:** Take a random sample.\n",
    "4.  **Calculate Statistic:** Find the Z, T, or Chi-Square value.\n",
    "5.  **Find the P-value:** Use a distribution table or software.\n",
    "6.  **Make a Decision:** Reject or Fail to Reject $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Errors in Testing (Type I & Type II)\n",
    "No test is perfect. We categorize mistakes into two types:\n",
    "\n",
    "| Error Type | Common Name | Definition | Analogy |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Type I Error** | False Positive | Rejecting $H_0$ when it was actually true. | Convicting an innocent person. |\n",
    "| **Type II Error** | False Negative | Failing to reject $H_0$ when $H_a$ was true. | Letting a guilty person go free. |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Use Cases in Data Science & ML\n",
    "* **A/B Testing:** Comparing a \"Control\" group and a \"Treatment\" group to see if a product change improved a metric.\n",
    "* **Feature Selection:** Using P-values to determine if a specific variable significantly helps a model predict the target.\n",
    "* **Model Performance:** Testing if a new model's accuracy is significantly higher than the baseline model's.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Python Implementation: Two-Sample T-test\n",
    "This is the most common test for comparing two groups (like an A/B test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e111077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistic: -6.3509\n",
      "P-Value: 0.0000\n",
      "Conclusion: Reject Null Hypothesis. Group B is significantly different.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# 1. Sample Data (Conversion rates for Group A vs Group B)\n",
    "group_a = [0.12, 0.15, 0.11, 0.14, 0.13, 0.16, 0.12, 0.11]\n",
    "group_b = [0.18, 0.19, 0.17, 0.20, 0.16, 0.21, 0.19, 0.18]\n",
    "\n",
    "# 2. Perform Independent Two-Sample T-test\n",
    "t_stat, p_val = stats.ttest_ind(group_a, group_b)\n",
    "\n",
    "# 3. Decision Logic\n",
    "alpha = 0.05\n",
    "print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "print(f\"P-Value: {p_val:.4f}\")\n",
    "\n",
    "if p_val < alpha:\n",
    "    print(\"Conclusion: Reject Null Hypothesis. Group B is significantly different.\")\n",
    "else:\n",
    "    print(\"Conclusion: Fail to Reject Null. No significant difference found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70476e21",
   "metadata": {},
   "source": [
    "# The Foundation of Testing: Null vs. Alternative Hypothesis\n",
    "\n",
    "In any statistical experiment, we must set up two competing statements. These hypotheses must be **mutually exclusive** (they cannot both be true) and **collectively exhaustive** (they cover all possible outcomes).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Null Hypothesis ($H_0$)\n",
    "The **Null Hypothesis** represents the default assumption that there is **no effect**, no change, or no difference.\n",
    "\n",
    "* **Logic:** We assume $H_0$ is true until the data provides overwhelming evidence to the contrary.\n",
    "* **Mathematical Sign:** Always contains a version of \"equality\" ($=$, $\\le$, or $\\ge$).\n",
    "* **Goal:** To be tested, with the hope of being \"rejected.\"\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Alternative Hypothesis ($H_a$ or $H_1$)\n",
    "The **Alternative Hypothesis** represents the claim we are trying to find evidence for. It is the \"challenger\" to the status quo.\n",
    "\n",
    "* **Logic:** If the data is highly unlikely to occur under $H_0$, we accept $H_a$.\n",
    "* **Mathematical Sign:** Always contains a version of \"inequality\" ($\\ne$, $>$, or $<$ ).\n",
    "* **Goal:** To provide evidence for a new theory or discovery.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. How to Set Them Up (3 Scenarios)\n",
    "\n",
    "The way you write your hypotheses depends on what you are looking for:\n",
    "\n",
    "| Test Type | Research Goal | Null Hypothesis ($H_0$) | Alternative ($H_a$) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Two-Tailed** | \"Is there *any* change?\" | $\\mu = \\text{Value}$ | $\\mu \\ne \\text{Value}$ |\n",
    "| **Right-Tailed** | \"Is it *greater* than?\" | $\\mu \\le \\text{Value}$ | $\\mu > \\text{Value}$ |\n",
    "| **Left-Tailed** | \"Is it *less* than?\" | $\\mu \\ge \\text{Value}$ | $\\mu < \\text{Value}$ |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Real-World DS/ML Examples\n",
    "\n",
    "### A/B Testing (Conversion Rate)\n",
    "* **$H_0$:** The new button design has the same conversion rate as the old one ($p_1 = p_2$).\n",
    "* **$H_a$:** The new button design has a different (or higher) conversion rate ($p_1 \\ne p_2$ or $p_1 > p_2$).\n",
    "\n",
    "### Regression Analysis (Feature Importance)\n",
    "* **$H_0$:** The weight (coefficient) of the \"Price\" feature is zero ($\\beta = 0$). *Meaning: Price doesn't help predict sales.*\n",
    "* **$H_a$:** The weight of the \"Price\" feature is not zero ($\\beta \\ne 0$). *Meaning: Price is a significant predictor.*\n",
    "\n",
    "---\n",
    "\n",
    "## 5. The Decision Matrix (The Outcomes)\n",
    "Once you have defined your hypotheses and run your test, you end up with one of two decisions:\n",
    "\n",
    "| If P-value is... | Action Taken | Interpretation |\n",
    "| :--- | :--- | :--- |\n",
    "| **Small ($\\le 0.05$)** | **Reject $H_0$** | \"Evidence supports the Alternative. There is a significant effect.\" |\n",
    "| **Large ($> 0.05$)** | **Fail to Reject $H_0$** | \"Evidence is insufficient. Any difference seen is likely random noise.\" |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Pro-Tip: \"Not Guilty\" vs. \"Innocent\"\n",
    "Never say \"We accept the Null Hypothesis.\" In statistics, as in law, we say **\"Fail to Reject.\"** * **Analogy:** A \"Not Guilty\" verdict doesn't prove a person is innocent; it simply means there wasn't enough evidence to prove they were guilty beyond a reasonable doubt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276b128",
   "metadata": {},
   "source": [
    "# Steps Involved in Hypothesis Testing\n",
    "\n",
    "Hypothesis testing follows a standardized, logical sequence to ensure that conclusions are based on statistical evidence rather than intuition.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The 5-Step Statistical Workflow\n",
    "\n",
    "### Step 1: State the Hypotheses ($H_0$ and $H_a$)\n",
    "Define the two competing claims.\n",
    "* **Null Hypothesis ($H_0$):** The assumption of no effect or no difference.\n",
    "* **Alternative Hypothesis ($H_a$):** The claim you want to support (the effect you're looking for).\n",
    "\n",
    "### Step 2: Choose the Significance Level ($\\alpha$)\n",
    "Decide the threshold for rejecting the null hypothesis before looking at the data. \n",
    "* **Standard:** $\\alpha = 0.05$ (5% risk of a Type I error).\n",
    "* **Stringent:** $\\alpha = 0.01$ (Used in medical or high-stakes trials).\n",
    "\n",
    "### Step 3: Select the Appropriate Test & Collect Data\n",
    "Choose the test based on your data type and known parameters:\n",
    "* **Z-test:** $\\sigma$ is known and $n$ is large.\n",
    "* **T-test:** $\\sigma$ is unknown (most common).\n",
    "* **Chi-Square:** For categorical data/proportions.\n",
    "* **ANOVA:** For comparing means across 3+ groups.\n",
    "\n",
    "\n",
    "\n",
    "### Step 4: Calculate the Test Statistic and P-Value\n",
    "* **Test Statistic:** A measure of how far your sample deviates from the null (e.g., how many standard errors away).\n",
    "* **P-Value:** The probability of getting your result if $H_0$ is actually true.\n",
    "\n",
    "### Step 5: Make a Decision\n",
    "Compare your **P-value** to your **Alpha ($\\alpha$)**:\n",
    "* **P $\\le \\alpha$:** Reject $H_0$. The result is \"Statistically Significant.\"\n",
    "* **P $> \\alpha$:** Fail to Reject $H_0$. The result is not significant.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Visualizing the Decision Region\n",
    "In a normal distribution, the $\\alpha$ defines the **Rejection Region**. If your test statistic falls in the shaded \"tail,\" you reject the null.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Python Implementation: The Complete Workflow\n",
    "This script simulates a test to see if a new machine produces parts with a different average weight than the target (100g).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a4e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mean: 102.49\n",
      "T-Statistic: 2.3020\n",
      "P-Value: 0.0303\n",
      "Decision: Reject H0 (Significant difference at alpha=0.05)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# 1. State Hypotheses\n",
    "# H0: mean = 100\n",
    "# Ha: mean != 100\n",
    "target_mean = 100\n",
    "alpha = 0.05\n",
    "\n",
    "# 2. Collect Data (Sample of 25 parts)\n",
    "np.random.seed(10)\n",
    "sample_data = np.random.normal(loc=102, scale=5, size=25)\n",
    "\n",
    "# 3. Calculate Test Statistic and P-Value (One-sample T-test)\n",
    "t_stat, p_value = stats.ttest_1samp(sample_data, target_mean)\n",
    "\n",
    "# 4. Decision\n",
    "print(f\"Sample Mean: {np.mean(sample_data):.2f}\")\n",
    "print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "\n",
    "if p_value <= alpha:\n",
    "    print(f\"Decision: Reject H0 (Significant difference at alpha={alpha})\")\n",
    "else:\n",
    "    print(f\"Decision: Fail to Reject H0 (No significant difference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5d59c",
   "metadata": {},
   "source": [
    "# The Rejection Region Approach (Critical Value Method)\n",
    "\n",
    "In the **Rejection Region Approach**, we define a specific range of values for the test statistic that are so unlikely to occur under the Null Hypothesis ($H_0$) that we agree to reject it if our calculated value falls within that range.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Key Terminology\n",
    "\n",
    "* **Critical Value:** The threshold \"cutoff\" point on the horizontal axis of the distribution. It is determined by the Significance Level ($\\alpha$).\n",
    "* **Rejection Region:** The area in the \"tails\" of the distribution. If your test statistic lands here, you reject $H_0$.\n",
    "* **Non-Rejection Region:** The area in the center of the distribution. If your test statistic lands here, you \"Fail to Reject\" $H_0$.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Steps in the Rejection Region Approach\n",
    "\n",
    "1.  **State Hypotheses:** Define $H_0$ and $H_a$.\n",
    "2.  **Specify Alpha ($\\alpha$):** Choose your significance level (e.g., 0.05).\n",
    "3.  **Find the Critical Value:** Look up the value ($Z_c$ or $t_c$) that corresponds to $\\alpha$ using a table or code.\n",
    "4.  **Calculate the Test Statistic:** Compute your Z or T score from the sample data.\n",
    "5.  **Make a Decision:** * If **$|Test Statistic| > |Critical Value|$**, it has fallen into the Rejection Region $\\rightarrow$ **Reject $H_0$**.\n",
    "    * Otherwise $\\rightarrow$ **Fail to Reject $H_0$**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Directional Regions (One-Tailed vs. Two-Tailed)\n",
    "\n",
    "The location of the rejection region depends on your Alternative Hypothesis ($H_a$):\n",
    "\n",
    "| Test Type | Alternative Hypothesis | Rejection Region Location |\n",
    "| :--- | :--- | :--- |\n",
    "| **Two-Tailed** | $H_a: \\mu \\ne \\mu_0$ | Both tails (Split $\\alpha$ into $\\alpha/2$ for each tail) |\n",
    "| **Right-Tailed** | $H_a: \\mu > \\mu_0$ | Upper (right) tail only |\n",
    "| **Left-Tailed** | $H_a: \\mu < \\mu_0$ | Lower (left) tail only |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Python Implementation\n",
    "This script shows how to find the Critical Value and compare it to a Test Statistic for a 95% confidence Z-test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74adad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Value: 1.960\n",
      "Calculated Z-Statistic: 2.150\n",
      "Decision: Reject H0 (The statistic fell in the rejection region)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# 1. Setup\n",
    "alpha = 0.05\n",
    "test_type = \"two-tailed\"\n",
    "\n",
    "# 2. Find Critical Value (Z*)\n",
    "if test_type == \"two-tailed\":\n",
    "    # For two-tailed, alpha is split (0.025 on each side)\n",
    "    z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "else:\n",
    "    z_critical = stats.norm.ppf(1 - alpha)\n",
    "\n",
    "# 3. Simulated Test Statistic (Calculated from your data)\n",
    "z_statistic = 2.15 \n",
    "\n",
    "print(f\"Critical Value: {z_critical:.3f}\")\n",
    "print(f\"Calculated Z-Statistic: {z_statistic:.3f}\")\n",
    "\n",
    "# 4. Decision Logic\n",
    "if abs(z_statistic) > z_critical:\n",
    "    print(\"Decision: Reject H0 (The statistic fell in the rejection region)\")\n",
    "else:\n",
    "    print(\"Decision: Fail to Reject H0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4f05e",
   "metadata": {},
   "source": [
    "\n",
    "##  Drawbacks of the Rejection Region Approach\n",
    "\n",
    "While useful for manual calculations, this approach has several limitations in a modern context:\n",
    "\n",
    "* **\"All-or-Nothing\" Decision:** It only tells you *if* you rejected the null, not *how strongly* you rejected it. A test statistic just barely inside the region is treated the same as one far out in the tail.\n",
    "* **Fixed Alpha ($\\alpha$):** It requires you to commit to a significance level before the test. If you want to see if the result would have been significant at $\\alpha = 0.01$ instead of $0.05$, you have to look up new critical values and start over.\n",
    "* **Loss of Information:** Unlike the P-value approach, it doesn't provide the exact probability of observing the data. This makes it harder to compare results across different studies.\n",
    "* **Binary Thinking:** It encourages a \"black and white\" view of significance. In reality, a result that falls just outside the rejection region might still be worth investigating, but this approach shuts the door on it.\n",
    "* **Table Dependency:** It often relies on statistical tables (Z-tables, T-tables), which are less efficient than the precise probabilities generated by modern software.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Python Comparison\n",
    "This script shows how the P-value provides more \"nuance\" than the binary Rejection Region check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f172c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Value: 1.960\n",
      "Is Rejected? True\n",
      "Exact P-Value: 0.0488\n",
      "------------------------------\n",
      "Observation: The Z-stat is BARELY in the rejection region.\n",
      "The P-value shows us just how close it was (0.0488 vs 0.0500).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Inputs\n",
    "alpha = 0.05\n",
    "z_stat = 1.97  # Our calculated statistic\n",
    "\n",
    "# Rejection Region Method (Binary)\n",
    "z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "is_rejected = abs(z_stat) > z_critical\n",
    "\n",
    "# P-Value Method (Nuanced)\n",
    "p_val = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "\n",
    "print(f\"Critical Value: {z_critical:.3f}\")\n",
    "print(f\"Is Rejected? {is_rejected}\")\n",
    "print(f\"Exact P-Value: {p_val:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Observation: The Z-stat is BARELY in the rejection region.\")\n",
    "print(\"The P-value shows us just how close it was (0.0488 vs 0.0500).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308ce81",
   "metadata": {},
   "source": [
    "# Errors in Hypothesis Testing: Type I and Type II\n",
    "\n",
    "When we make a decision to Reject or Fail to Reject the Null Hypothesis ($H_0$), there are four possible outcomes. Two are correct decisions, and two are errors.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Error Matrix (Truth vs. Decision)\n",
    "\n",
    "| | $H_0$ is Actually True | $H_0$ is Actually False |\n",
    "| :--- | :--- | :--- |\n",
    "| **Decision: Reject $H_0$** | **Type I Error** (False Positive) | **Correct Decision** (Power) |\n",
    "| **Decision: Fail to Reject $H_0$** | **Correct Decision** | **Type II Error** (False Negative) |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Type I Error (False Positive)\n",
    "A Type I error occurs when we **reject a Null Hypothesis that is actually true**. We claim there is an effect or a difference when there isn't one.\n",
    "\n",
    "* **Probability:** The probability of a Type I error is exactly equal to the **Significance Level ($\\alpha$)**. \n",
    "* **Symbol:** $\\alpha$ (Alpha).\n",
    "* **Example:** A medical test tells a patient they have a disease when they are actually healthy.\n",
    "* **Control:** You reduce Type I error by choosing a smaller alpha (e.g., 0.01 instead of 0.05).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Type II Error (False Negative)\n",
    "A Type II error occurs when we **fail to reject a Null Hypothesis that is actually false**. We miss a real effect or difference.\n",
    "\n",
    "* **Probability:** Represented by the symbol $\\beta$ (Beta).\n",
    "* **Symbol:** $\\beta$.\n",
    "* **Power:** The ability of a test to avoid a Type II error is called **Statistical Power** ($1 - \\beta$).\n",
    "* **Example:** A medical test tells a sick patient they are healthy, missing the disease.\n",
    "* **Control:** You reduce Type II error by increasing your **sample size ($n$)** or increasing the effect size.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. The Trade-off: The Seesaw Effect\n",
    "Type I and Type II errors are inversely related. If you try to strictly avoid one, you increase the risk of the other.\n",
    "\n",
    "* **If you decrease $\\alpha$ (make it harder to reject $H_0$):** You lower the risk of a False Positive, but you increase the risk of a False Negative ($\\beta$).\n",
    "* **If you increase $\\alpha$ (make it easier to reject $H_0$):** You increase the risk of a False Positive, but you are more likely to catch real effects (lower $\\beta$).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Which Error is Worse?\n",
    "It depends entirely on the **context** of the problem:\n",
    "\n",
    "* **Criminal Trial:** A Type I error (convicting an innocent person) is usually considered worse than a Type II error (acquitting a guilty person).\n",
    "* **Medical Screening:** A Type II error (missing a cancer diagnosis) is usually considered much worse than a Type I error (a false alarm that leads to more tests).\n",
    "* **Spam Filters:** A Type I error (sending an important work email to the spam folder) is worse than a Type II error (letting a single junk email into the inbox).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Python: Visualizing the Impact of Alpha\n",
    "This code demonstrates how changing the significance level shifts the threshold and changes the probability of both errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4afe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significance Level (Alpha): 0.05\n",
      "Critical Value threshold: 1.64\n",
      "Probability of Type II Error (Beta): 0.0877\n",
      "Statistical Power (1 - Beta): 0.9123\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Parameters\n",
    "mu_null = 0    # Mean under H0\n",
    "mu_alt = 3     # Mean under Ha (Actual effect)\n",
    "std = 1        # Standard deviation\n",
    "alpha = 0.05   # Type I Error limit\n",
    "\n",
    "# Calculate Critical Value\n",
    "critical_val = norm.ppf(1 - alpha, mu_null, std)\n",
    "\n",
    "# Calculate Beta (Type II Error)\n",
    "# Beta is the area under the Alt distribution to the LEFT of the critical value\n",
    "beta = norm.cdf(critical_val, mu_alt, std)\n",
    "power = 1 - beta\n",
    "\n",
    "print(f\"Significance Level (Alpha): {alpha}\")\n",
    "print(f\"Critical Value threshold: {critical_val:.2f}\")\n",
    "print(f\"Probability of Type II Error (Beta): {beta:.4f}\")\n",
    "print(f\"Statistical Power (1 - Beta): {power:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d161a",
   "metadata": {},
   "source": [
    "# One-Tailed vs. Two-Tailed Hypothesis Tests\n",
    "\n",
    "The choice between a one-tailed and two-tailed test depends on the research question and whether you are looking for a change in a **specific direction** or **any change** at all.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Structural Differences\n",
    "\n",
    "### Two-Tailed Test (Two-Sided)\n",
    "Used when you want to detect a difference in **either direction** (increase or decrease). \n",
    "* **Null Hypothesis ($H_0$):** $\\mu = \\mu_0$\n",
    "* **Alternative Hypothesis ($H_a$):** $\\mu \\neq \\mu_0$\n",
    "* **Rejection Region:** Split into two equal halves at both ends of the distribution.\n",
    "\n",
    "\n",
    "\n",
    "### One-Tailed Test (One-Sided)\n",
    "Used when you are only interested in a change in **one specific direction**.\n",
    "* **Right-Tailed:** $H_a: \\mu > \\mu_0$ (Testing for an increase)\n",
    "* **Left-Tailed:** $H_a: \\mu < \\mu_0$ (Testing for a decrease)\n",
    "* **Rejection Region:** Located entirely in one tail.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Comparison: Advantages & Disadvantages\n",
    "\n",
    "| Test Type | Advantages | Disadvantages |\n",
    "| :--- | :--- | :--- |\n",
    "| **Two-Tailed** | **Unbiased:** Detects effects in both directions. Safer and more conservative. Standard for academic research. | **Lower Power:** Harder to reject the Null because the significance level ($\\alpha$) is split (e.g., 0.025 in each tail). |\n",
    "| **One-Tailed** | **Higher Power:** Easier to detect a significant result in the predicted direction because all of $\\alpha$ is in one tail. | **Risky:** If the effect happens in the *opposite* direction of your prediction, you must ignore it (cannot reject $H_0$). |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. When to Use Which?\n",
    "\n",
    "### Use a Two-Tailed Test if:\n",
    "* You are exploring a new area and don't know what to expect.\n",
    "* You want to be scientifically rigorous (e.g., \"Does this new drug affect blood pressure at all?\").\n",
    "* You want to avoid the \"cherry-picking\" bias.\n",
    "\n",
    "### Use a One-Tailed Test if:\n",
    "* An effect in the opposite direction is physically impossible or meaningless.\n",
    "* You are specifically testing for improvement (e.g., \"Does this training increase scores?\").\n",
    "* You have strong theoretical or historical evidence that the effect only goes one way.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. The Mathematical Trade-off\n",
    "For a significance level of $\\alpha = 0.05$:\n",
    "* In a **One-Tailed Test**, you need a Z-score of **1.645** to reach the rejection region.\n",
    "* In a **Two-Tailed Test**, you need a higher Z-score of **1.96** to reach the rejection region.\n",
    "\n",
    "**Conclusion:** One-tailed tests make it \"easier\" to find significance, but only if you are right about the direction.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Python Implementation\n",
    "This code demonstrates how the same data results in different P-values depending on the \"tails\" chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebc42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-Score: 2.0\n",
      "Two-Tailed P-value: 0.0455 (Significant at 0.05)\n",
      "One-Tailed P-value: 0.0228 (Highly Significant)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data: Sample mean is 105, Pop mean is 100\n",
    "sample_mean = 105\n",
    "pop_mean = 100\n",
    "std_err = 2.5\n",
    "z_score = (sample_mean - pop_mean) / std_err # Z = 2.0\n",
    "\n",
    "# 1. Two-Tailed P-value\n",
    "p_two_tailed = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "\n",
    "# 2. One-Tailed P-value (Right-tailed)\n",
    "p_one_tailed = 1 - stats.norm.cdf(z_score)\n",
    "\n",
    "print(f\"Z-Score: {z_score}\")\n",
    "print(f\"Two-Tailed P-value: {p_two_tailed:.4f} (Significant at 0.05)\")\n",
    "print(f\"One-Tailed P-value: {p_one_tailed:.4f} (Highly Significant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ddb52",
   "metadata": {},
   "source": [
    "# Hypothesis Testing in Data Science & ML\n",
    "\n",
    "While ML often focuses on predictive accuracy, Hypothesis Testing focuses on **validity** and **inference**. We use it to decide which features to keep, which models to trust, and which business changes to implement.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Feature Selection and Engineering\n",
    "In the early stages of building a model, we use hypothesis tests to determine if a feature actually has a relationship with the target variable.\n",
    "\n",
    "* **P-values in Linear Regression:** Every coefficient in a regression model comes with a P-value. \n",
    "    * **$H_0$:** The feature's coefficient is 0 (it has no effect).\n",
    "    * **$H_a$:** The feature's coefficient is not 0 (it is a significant predictor).\n",
    "* **Chi-Square Test:** Used to check for independence between categorical variables (e.g., \"Does a user's country affect their likelihood to subscribe?\").\n",
    "* **ANOVA:** Used to see if there are significant differences in a numerical target across multiple categories.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Performance Comparison\n",
    "Is Model B actually better than Model A, or did it just get lucky on the test set?\n",
    "\n",
    "* **Paired T-test:** When comparing two models (like a Random Forest vs. an XGBoost) on the same K-Fold cross-validation splits. If the P-value is small, we can confidently say one model is statistically superior.\n",
    "* **McNemar’s Test:** A specific test used to compare the error rates of two classifiers by looking at where they made different mistakes.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Product Analytics & A/B Testing\n",
    "This is the most common use of hypothesis testing in the tech industry.\n",
    "\n",
    "* **The Scenario:** You change the \"Buy Now\" button from blue to green.\n",
    "* **$H_0$:** There is no difference in click-through rate (CTR).\n",
    "* **$H_a$:** The green button has a higher CTR.\n",
    "* **The Test:** A **Two-Sample Z-test for Proportions** is used to determine if the 2% lift you saw is real or just a random fluctuation.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Checking Model Assumptions\n",
    "Many ML algorithms (especially linear ones) only work correctly if the data meets certain criteria. We use hypothesis tests to verify these:\n",
    "\n",
    "* **Shapiro-Wilk Test:** Checks if your data is **Normally Distributed**.\n",
    "* **Levene’s Test:** Checks for **Homoscedasticity** (equal variance) between groups.\n",
    "* **Durbin-Watson Test:** Checks for **Autocorrelation** in time-series data.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Detecting Data Drift\n",
    "In production, a model's performance can \"decay\" because the incoming real-world data starts looking different from the training data.\n",
    "\n",
    "* **Kolmogorov-Smirnov (K-S) Test:** Used to compare the distribution of features in production vs. training. If the K-S test rejects the null, it means the data has shifted, and the model needs to be retrained.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary: Which Test for Which Task?\n",
    "\n",
    "| Task | Statistical Test |\n",
    "| :--- | :--- |\n",
    "| **Selecting Features** | P-values (T-tests/F-tests) |\n",
    "| **Comparing 2 Group Means** | Independent T-test |\n",
    "| **Comparing 3+ Group Means** | ANOVA |\n",
    "| **Checking Normality** | Shapiro-Wilk Test |\n",
    "| **Categorical Dependencies** | Chi-Square Test |\n",
    "| **Comparing Model Errors** | McNemar’s Test |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f5082",
   "metadata": {},
   "source": [
    "source link - [https://drive.google.com/file/d/1J6TWERqWu1-98n2b8uBKdU8j0aCVgyuN/view]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
